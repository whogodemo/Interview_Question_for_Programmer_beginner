# 머신러닝 면접 질문 리스트

* 인공지능과 머신러닝의 차이

인공지능이 머신러닝과 딥러닝을 포함하는 상위개념이다.

초기 인공지능은 직접 사람이 체계를 조직화하고 설계하여 인간 수준의 지능을 구현하기 위해 노력했다.
프로그래머가 모든 알고리즘을 설계하고 조직화하여 인간 수준의 인공지능을 만들수 있다고 믿었다.

머신 러닝은 이보다 더 발전된 형태이다.
인간이 설계한 알고리즘으로 특징을 추출하고, 이후 `특징의 분포를 기계가 학습`하여 결과를 판단하는 것이다.
인간이 직접 모든 알고리즘을 설계하고 조직화하던 인공지능과 달리
인간이 `특징 추출`까지만 알고리즘을 설계하고, 이후는 기계가 `직접 학습`을 한다는 것에서 차이점이 있다.
인공지능에 비해 머신러닝에서 프로그래머의 개입이 줄어드는 것이다.


* 머신러닝과 딥러닝의 차이

머신러닝 또한 딥러닝을 포함하는 상위개념이다.
마찬가지로, 인공지능 > 머신러닝 > 딥러닝 하위 개념으로 갈수록 프로그래머의 개입이 줄어든다.

파라미터가 결과에 미치는 영향을 쉽게 이해할수 있으면 머신러닝이라고 한다.
그러나 파라미터가 결과에 미치는 영향을 쉽게 이해할 수 없으면 딥러닝이라고 한다.
예를 들면 '암 발병률'을 예측할 때, 
암 발병률 = a * 운동 + b * 유전
이런식으로 나타낼 수 있기 때문에 운동과 유전이 암 발병률에 미치는 영향을 쉽게 이해할 수 있다.
따라서 머신러닝에 속한다.
그러나 딥러닝의 경우 파라미터에 따라 결과가 어떻게 변하는지 이해할 수 없다.

* 지도학습과 비지도학습의 차이

지도학습에는 인풋 데이터에 대한 정답 아웃풋을 제공하지만,
비지도학습의 경우 정답을 알려주지 않고 기계가 직접 패턴을 추출한다.
따라서 지도학습의 경우 `정답이 있는 데이터`를 학습하고, 비지도학습의 경우에는 `정답이 없는 데이터`를 학습한다.

분류와 회귀는 지도학습에 포함되고, 군집화는 비지도학습에 포함된다.

* 강화학습

지도학습과 비지도학습의 중간에 해당한다.
문제에 대한 직접적인 답을 주지는 않지만 보상을 줌으로써 보상이 최대가 되는 정책을 찾는다.
예를 들면 알파고, 게임 학습이 해당된다.

* 회귀(Regression)와 분류(Classification)의 차이

회귀는 연속적인 실수 값을 예측하는 문제이고,
분류는 여러개 클래스 중에서 하나를 선택하는 문제이다.

예를 들면 회귀는 '사람의 키가 주어졌을 때 몸무게를 예측'하는 것처럼 범위가 있는 문제에 해당된다.
분류는 '개인지 고양이인지' 선택하는 문제에 해당된다.

* 정규화
* 결정트리
* 교차검증

* 하이퍼파라미터와 파라미터

파라미터란 학습에 의해 변경되는 딥러닝의 필터 값이고,
하이퍼파라미터는 사람이 직접 정의해줘야 한다.
손실함수와 활성화 함수, 레이어 개수, 데이터 전처리 방법, 이니셜라이저 세팅, 모델 선택과 노드 개수들이 포함된다.

# 딥러닝

딥러닝이란 결국 필터를 학습하는 것이다. 입력과 출력이 정해져있을 때, 해당 입력으로부터 거의 비슷한 출력을 만들어내기 위해 가중치와 편향(parameter)를 학습하는 것이다.
인공신경망에 입력을 넣은 결과가 실제 출력과 얼마나 차이가 있는지를 보고 파라미터를 계속 조정한다.
실제 출력과의 차이를 알아내는 것은 손실 함수(cost function)가 담당하고, 파라미터는 역전파(back propagation algorithm)에 의해 조정된다.


* 손실함수

출력 결과와 실제 정답 출력과의 차이를 계산하는 함수이다.
cross-entropy 에러와 mean-squared 에러 등 다양한 방법이 있다.

* 활성화함수

인공신경망에서 입력 신호의 총합을 출력 신호로 변환하는 함수이다. 입력값에 따라 다음 신호로 내보낼지 말지(activate)를 결정하기 때문에 활성화함수이다.
활성화함수의 종류는 Relu, sigmoid, 소프트맥스 등 다양한 함수가 있지만 공통적으로는 모두 비선형 함수이다.
활성화함수가 선형함수이면 신경망을 여러겹 쌓는 이유가 없어지기 때문이다. 신경망이 한겹인것과 똑같은 결과를 낸다.

* optimizer(최적화)

손실이 최소가 되는 최적의 가중치와 편향을 찾아내는 것을 최적화라고 한다. 최적화의 한 방법으로 grdient descent가 있다.

* gradient descent

손실을 최소화하기 위해 가장 가파른 방향으로 움직이는 것을 gradient descent라고 한다.
gradient의 음의 방향으로 진행해서 gradient descent이다.
발전된 방식으로 확률적 경사하강법, 미니배치 경사하강법 등이 있다.

* 역전파

## CNN

* 간단히 설명하면?

이미지 인식과 분류 학습에 뛰어난 딥러닝.

이미지와 해당 이미지의 정답이 적힌 데이터가 있을때
계속 학습하면서 이미지를 분류하는 필터를 계속 조정해나가는 것이다.
예를 들면 개, 고양이 이미지와 그 정답만 주어진다면
개와 고양이 이미지를 분류하는 필터를 학습할 수 있다.

* CNN과 기타 알고리즘의 차이(장단점)

모든 뉴런이 연결되어 있지 않다.(not fully-connected)
몇 개의 인접한 뉴런만 연결되어 있기 때문에 학습 시간을 줄일 수 있고 가중치의 개수를 줄일 수 있다.

* CNN을 사용하여 프로젝트를 진행한 이유

RNN 및 lstm을 이용하여 프로젝트를 진행하는 것도 고려했고,
실제로 학습을 진행했으나 학습 시간이 매우 길었다. 하나의 단어에 대한 20개의 영상을 트레이닝하는데만 3일 이상의 시간이 소요됐다.
게다가 학습이 완료된다고 해도, 가중치의 개수 자체가 CNN보다 많기 때문에 속도 면에서 실시간과는 적합하지 않다고 판단했다.
따라서 비용과 시간의 문제로 CNN을 선택했다.

## RNN/ LSTM

* 간단히 설명하면?

재귀신경망으로, 시퀀스 데이터를 학습한다.
CNN의 경우 하나의 인풋에서 하나의 아웃풋을 출력하는 구조이기 때문에 시퀀스 데이터 처리에는 적합하지 않다.
이미지는 학습할 수 있어도 영상은 학습할 수 없는 것이다.

시퀀스 데이터를 학습하기 위해 과거의 학습 결과를 현재의 학습에 반영한다.

* RNN과 Lstm의 차이

RNN의 경우 시퀀스 데이터의 길이가 길어질수록 gradient vanish problem이 발생한다.
이를 해결하기 위해 Lstm을 사용한다. Lstm은 forget gate가 있어서, 이전 셀의 정보를 잊을지 기억할지를 결정할 수 있다.
forget gate가 1이 될 수 있기 때문에 gradient vanishing problem을 막을 수 있다.

## Recommendation Algorithm

* 간단히 설명하면?

* contents-based filtering
* collaborative filtering

## GAN 생성적 적대 신경망

* 간단히 설명하면?

* generator
* discriminator

## 자연어처리

* 간단히 설명하면?


## etc

만약 데이터가 주어진다면, 그 데이터를 학습하기 위해 모델 구성을 어떻게 할것인지.

이외에도 머신러닝 분야에 대해 공부해본 것이 있는지

데이터 모델링이란?

## reference
https://tensorflow.blog/%EC%BC%80%EB%9D%BC%EC%8A%A4-%EB%94%A5%EB%9F%AC%EB%8B%9D/1-%EB%94%A5%EB%9F%AC%EB%8B%9D%EC%9D%B4%EB%9E%80-%EB%AC%B4%EC%97%87%EC%9D%B8%EA%B0%80/
